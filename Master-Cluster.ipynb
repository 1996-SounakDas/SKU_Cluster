{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNN5slzf6lv3"
      },
      "source": [
        "# Clustering in SKU Segmentation\n",
        "This is an attempt to pipeline the model. The aim is to obtain the best algorithm based on the external performance evaluation metrics as it can be realised upon looking in the data that there is a lack of ground truth.\n",
        "\n",
        "The script utilizes a free software machine learning library “scikit-learn” as a core\n",
        "complementing it with several algorithms.\n",
        "The script uses the concept of data-pipeline to consequentially perform the following procedures:\n",
        "1. to impute the missing data with Multivariate Imputation\n",
        "2. to standardize the data\n",
        "3. to identify and trim outliers and small 'blobs' with Isolation Forest\n",
        "4. to cluster the data with k-mean, BIRCH and Affinity Propagation\n",
        "5. to improve the eventual clustering result via PCA\n",
        "\n",
        "Since the ground truth is not provided, the clustering is validated only by internal evaluation, namely\n",
        "by silhouette index, Calinski-Harabazs index and Davies-Bouldin '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EPSAwWL7C0k"
      },
      "source": [
        "## Import the necessary libraries\n",
        "The libraries that are being used are imported at the beginning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f-E1ag26dYr"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import IterativeImputer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from itertools import cycle\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AffinityPropagation,Birch\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import davies_bouldin_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWGXLxlo7oaB"
      },
      "source": [
        "## Pipelining Class\n",
        "This ensures that the methods are being piped into the model as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CEIORlB47nqD"
      },
      "source": [
        "class Pipeline:\n",
        "\n",
        "    def __init__(self, methods):\n",
        "        self.methods = methods\n",
        "\n",
        "    def pump(self):\n",
        "        for method in self.methods:\n",
        "            method"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuN3vTmS8kge"
      },
      "source": [
        "## Data Pre-Processing\n",
        "This stage has the functions that are responsible for Data imputation (using Multivariate Imputation using Chained Equation), Standardization (using MaxAbsScaler) and Outlier Identification (using Isolation Forest). The data obtained will be void of any outliers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLqqPDbr724S"
      },
      "source": [
        "class Processing:\n",
        "\n",
        "    def __init__(self, data, initial_strategy = 'mean', max_features=6):\n",
        "        self.data = data\n",
        "        self.initial_strategy = initial_strategy\n",
        "        self.max_features = max_features\n",
        "\n",
        "    def mice(self, initial_strategy):\n",
        "        self.data = pd.DataFrame(IterativeImputer(self.k)).fit_transform(self.data)\n",
        "\n",
        "    def standardization(self):\n",
        "        self.data = preprocessing.MaxAbsScaler(self.data).fit_transform(self.data)\n",
        "\n",
        "    def outlier_removal(self,max_features):\n",
        "        iso = IsolationForest(self.max_features)\n",
        "        predicted = iso.fit_predict(self.data)\n",
        "        index = []\n",
        "        for i in range(len(predicted)):\n",
        "            if predicted == -1:\n",
        "                index.append(i)\n",
        "\n",
        "        print(len(index), \" outliers are found\")\n",
        "\n",
        "        self.data = np.delete(self.data,index,0)\n",
        "    def get_data(self):\n",
        "        return self.data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08JDAdso9Dtb"
      },
      "source": [
        "## Feature Reduction\n",
        "This stage has the functions that are responsible for dimensionality reduction. Upon conducting variance analysis, 2 factors account for 95% of the variance associated with the features. So, the number of components are kept 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1CzjYro72zJ"
      },
      "source": [
        "class Reduction:\n",
        "\n",
        "    def __init__(self, n_components=2):\n",
        "        self.n_components = n_components\n",
        "\n",
        "    def pca(self, data):\n",
        "        compressor = PCA(self.n_components)\n",
        "        compressor.fit(data)\n",
        "        return compressor.transform(data), compressor.explained_variance_ratio_.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lzai0uu9XLJ"
      },
      "source": [
        "## Clustering\n",
        "Algorithms like K-means, BIRCH and Affinity Propagation are implemented in these functions. It is linked to the evaluation metrics and the plotter functions which will plot the distributions of the 2 components with respect to each other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zge3Lhzn72u8"
      },
      "source": [
        "class Clustering:\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def k_means_clustering(self, k, plot_best=True, compress=0):\n",
        "        if compress != 0:\n",
        "            r = Reduction(compress)\n",
        "            self.data, variance = r.pca(self.data)\n",
        "            print(variance)\n",
        "\n",
        "        km = KMeans(n_clusters=k, random_state=0).fit(self.data)\n",
        "        labels = km.predict(self.data)\n",
        "\n",
        "        if plot_best is True:\n",
        "            red = Reduction(n_components=2)\n",
        "            to_plot, variance = red.pca(self.data)\n",
        "            labels_unique = np.unique(labels)\n",
        "            n_clusters_ = len(labels_unique)\n",
        "            plot = Plot()\n",
        "            plot.plot_clustering(to_plot, n_clusters_, labels, variance, 'K-means')\n",
        "\n",
        "        e = Evaluation(self.data, labels)\n",
        "        print(k, '-means')\n",
        "        e.evaluate()\n",
        "\n",
        "    def birch(self, k, thresh, brnc_fac, plot_best=True, compress=0):\n",
        "        if compress != 0:\n",
        "            r = Reduction(compress)\n",
        "            self.data, variance = r.pca(self.data)\n",
        "            print(variance)\n",
        "\n",
        "        brc = Birch(n_clusters=k, threshold = thresh, branching_factor = brnc_fac, random_state=0).fit(self.data)\n",
        "        labels = brc.predict(self.data)\n",
        "\n",
        "        if plot_best is True:\n",
        "            red = Reduction(n_components=2)\n",
        "            to_plot, variance = red.pca(self.data)\n",
        "            labels_unique = np.unique(labels)\n",
        "            n_clusters_ = len(labels_unique)\n",
        "            plot = Plot()\n",
        "            plot.plot_clustering(to_plot, n_clusters_, labels, variance, 'BIRCH')\n",
        "\n",
        "        e = Evaluation(self.data, labels)\n",
        "        print(BIRCH)\n",
        "        e.evaluate()\n",
        "\n",
        "    def affinity(self, damp, plot_best=True, compress=0):\n",
        "        if compress != 0:\n",
        "            r = Reduction(compress)\n",
        "            self.data, variance = r.pca(self.data)\n",
        "            print(variance)\n",
        "\n",
        "        ap = AffinityPropagation(damping = damp).fit(self.data)\n",
        "        labels = ap.predict(self.data)\n",
        "\n",
        "        if plot_best is True:\n",
        "            red = Reduction(n_components=2)\n",
        "            to_plot, variance = red.pca(self.data)\n",
        "            labels_unique = np.unique(labels)\n",
        "            n_clusters_ = len(labels_unique)\n",
        "            plot = Plot()\n",
        "            plot.plot_clustering(to_plot, n_clusters_, labels, variance, 'Affinity Propagation')\n",
        "\n",
        "        e = Evaluation(self.data, labels)\n",
        "        print(Affinity Propagation)\n",
        "        e.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pb7CdtxFDGOp"
      },
      "source": [
        "## Evaluation Metrics\n",
        "These are linked with each of the clustering algorithm. Here due to the absence of any sort of categorical variables or any variables indicating the \"ground truth\", internal evaluation metrics like Silhouette coefficient, Calinski-Harabasz Index and Davies-Bouldin Index are used to measure the effectiveness of the clustering. These metrics measure the inter- and intra-cluster distances by using different approaches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEU9Q3_772qt"
      },
      "source": [
        "class Evaluation:\n",
        "\n",
        "    def __init__(self, data, labels, metric='euclidean'):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.metric = metric\n",
        "\n",
        "    def silhouette(self):\n",
        "        return metrics.silhouette_score(self.data, self.labels, metric=self.metric)\n",
        "\n",
        "    def calinski_harabaz(self):\n",
        "        return metrics.calinski_harabaz_score(self.data, self.labels)\n",
        "\n",
        "    def dunn_index(self):\n",
        "        def normalize_to_smallest_integers(labels):\n",
        "            max_v = len(set(labels))\n",
        "\n",
        "            sorted_labels = np.sort(np.unique(labels))\n",
        "            unique_labels = range(max_v)\n",
        "            new_c = np.zeros(len(labels), dtype=np.int32)\n",
        "            for i, clust in enumerate(sorted_labels):\n",
        "                new_c[labels == clust] = unique_labels[i]\n",
        "            return new_c\n",
        "\n",
        "        def Davies_Bouldin(labels, distances):\n",
        "            return metrics.davies_bouldin_score(self.data,self.labels)\n",
        "\n",
        "    def evaluate(self):\n",
        "        coeff = ['Silhouette: ', self.silhouette(), 'Calinski-Harabaz: ',\n",
        "                 self.calinski_harabaz(), 'Davies Bouldin: ', self.Davies_Bouldin()]\n",
        "        print(coeff)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sF1M7Hx7EJOr"
      },
      "source": [
        "## Plotter Function\n",
        "These functions are used to plot the distribution of the two principal components obtained after principal component analysis. This helps to visualize the clustering that is obtained. After plotting the distributions of the plot i a scatter plot, colouring is used to denote the clusters that are obtained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4g_UPYo72lI"
      },
      "source": [
        "class Plot:\n",
        "\n",
        "    def plot_clustering(self, data, n_clusters_, labels, variance, name):\n",
        "        plt.figure()\n",
        "        plt.rc('font', size=10)\n",
        "        colors = cycle('bgrcmykbgrcmykbgrcmykbgrcmyk')\n",
        "        for k, col in zip(range(len(u_label3)), colors):\n",
        "                my_members = labels == k\n",
        "                plt.plot(d3[my_members, 1], d3[my_members, 0], col + '.')\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.title('Algoritm: {} Number of clusters: {}. \\n'\n",
        "                  '{}% of variance is preserved after PCA'.format(name, n_clusters_, round(variance*100, 2)))\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fof9DZTmEgtt"
      },
      "source": [
        "## Uploading the File\n",
        "Here, pandas dataframes are used to make the files more readable in the context of the interpreter. The initial variables are dropped as they are either categorical in nature or do not contribute any meaningful insights on data (like ID). The missing values denoted by 0.0 are replaced by the numpy character \"NaN\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psL17s-i72dC"
      },
      "source": [
        "file = pd.read_excel(io.stringIO(uploaded['initial_data.xlsx']))\n",
        "data = file.parse(\"Sheet1\")\n",
        "data[['Unitprice', 'Expire date', 'Outbound number','Total outbound','Pal grossweight', 'Pal height',\n",
        "      'Units per pal']] = data[['Unitprice', 'Expire date', 'Outbound number','Total outbound','Pal grossweight',\n",
        "                                'Pal height', 'Units per pal']].replace(0.0, np.nan)\n",
        "data = data.drop([\"ID\", \"Tradability\",  \"Init status\"], axis=1)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-Cekp9kFHMM"
      },
      "source": [
        "It finds the number of missing values for each feature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kA1huJU8MVK"
      },
      "source": [
        "data.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeeHpKOxFNnZ"
      },
      "source": [
        "Calculating the pearson's correlation amongst the features based on the data that is present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMC9r63v8O-H"
      },
      "source": [
        "data.corr(method ='pearson')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehk_8yfjFUga"
      },
      "source": [
        "Two features Total Outbound and Outbound number has a very strong linear correlation as obtained from the previous exercise of calculating the correlation and Evan's classification is used to obtain the relative strength of the linear correlation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgXrGYLx8VbY"
      },
      "source": [
        "data['Outbound Fraction'] = data['Total outbound']/data['Outbound number']\n",
        "data = data.drop(['Outbound number','Total outbound'],axis = 1)\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS3aZI22Frgt"
      },
      "source": [
        "Calculating the correlation amongst the features that are obtained after the previous exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zYhwhYt8Xzz"
      },
      "source": [
        "data.corr(method ='pearson')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb1UVbssF9q7"
      },
      "source": [
        "## Inputing the hyperparameter features\n",
        "Here, the parameter values used in the algorithms are input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SB09YzoX8eFe"
      },
      "source": [
        "p = Processing(data, 6)\n",
        "preprocessing_methods = [p.mice(), p.standardization(), p.outlier_removal()]\n",
        "pipe1 = Pipeline(preprocessing_methods)\n",
        "pipe1.pump()\n",
        "c = Clustering(p.get_data())\n",
        "pipe2 = Pipeline([c.k_means_clustering(8, plot_best=True, compress=2)])\n",
        "pipe2.pump()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}